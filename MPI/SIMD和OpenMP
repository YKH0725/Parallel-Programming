#include <iostream>
#include <mpi.h>
#include <sys/time.h>
#include <pmmintrin.h>
#include <omp.h>

using namespace std;

static const int N = 1000;
static const int thread_count = 4;

float arr[N][N];
float A[N][N];

void init_A(float arr[][N])
{
    for (int i = 0; i < N; i++)
    {
        for (int j = 0; j < N; j++)
        {
            arr[i][j] = 0;
        }
        arr[i][i] = 1.0;
        for (int j = i + 1; j < N; j++)
            arr[i][j] = rand() % 100;
    }

    for (int i = 0; i < N; i++)
    {
        int k1 = rand() % N;
        int k2 = rand() % N;
        for (int j = 0; j < N; j++)
        {
            arr[i][j] += arr[0][j];
            arr[k1][j] += arr[k2][j];
        }
    }
}




void reset_A(float A[][N], float arr[][N])
{
    for (int i = 0; i < N; i++)
        for (int j = 0; j < N; j++)
            A[i][j] = arr[i][j];
}


void print_A(float A[][N])
{
    for (int i = 0; i < N; i++)
    {
        for (int j = 0; j < N; j++)
            cout << A[i][j] << " ";
        cout << endl;
    }
    cout << endl;
}

void f_ordinary()
{
    reset_A(A, arr);
    timeval t_start;
    timeval t_end;
    gettimeofday(&t_start, NULL);

    for (int k = 0; k < N; k++)
    {
        for (int j = k + 1; j < N; j++)
        {
            A[k][j] = A[k][j] * 1.0 / A[k][k];
        }
        A[k][k] = 1.0;

        for (int i = k + 1; i < N; i++)
        {
            for (int j = k + 1; j < N; j++)
            {
                A[i][j] = A[i][j] - A[i][k] * A[k][j];
            }
            A[i][k] = 0;
        }
    }
    gettimeofday(&t_end, NULL);
    cout << "ordinary time cost: "
        << 1000 * (t_end.tv_sec - t_start.tv_sec) +
        0.001 * (t_end.tv_usec - t_start.tv_usec) << "ms" << endl;
}

void f_mpi_sse()
{
    timeval t_start, t_end;
    int num_proc, rank;

    MPI_Comm_size(MPI_COMM_WORLD, &num_proc);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);

    int block = N / num_proc;
    int remain = N % num_proc;

    if (rank == 0) {
        reset_A(A, arr);
        gettimeofday(&t_start, NULL);
        
        // 任务划分
        for (int i = 1; i < num_proc; i++) {
            int start = i * block;
            int count = (i == num_proc - 1) ? block + remain : block;
            MPI_Send(&A[start], count * N, MPI_FLOAT, i, 0, MPI_COMM_WORLD);
        }

        // SSE优化的LU分解
        LU_sse(A, rank, num_proc);

        // 收集结果
        for (int i = 1; i < num_proc; i++) {
            int start = i * block;
            int count = (i == num_proc - 1) ? block + remain : block;
            MPI_Recv(&A[start], count * N, MPI_FLOAT, i, 1, 
                    MPI_COMM_WORLD, MPI_STATUS_IGNORE);
        }

        gettimeofday(&t_end, NULL);
        cout << "Block MPI LU with SSE time cost: "
             << 1000 * (t_end.tv_sec - t_start.tv_sec) +
            0.001 * (t_end.tv_usec - t_start.tv_usec) << "ms" << endl;
    }
    else {
        // 接收分配到的数据块
        int start = rank * block;
        int count = (rank == num_proc - 1) ? block + remain : block;
        MPI_Recv(&A[start], count * N, MPI_FLOAT, 0, 0, 
                MPI_COMM_WORLD, MPI_STATUS_IGNORE);

        // SSE优化的LU分解
        LU_sse(A, rank, num_proc);

        // 发送结果回主进程
        MPI_Send(&A[start], count * N, MPI_FLOAT, 0, 1, MPI_COMM_WORLD);
    }
}

// SSE优化的LU分解函数
void LU_sse(float** A, int rank, int num_proc)
{
    int block = N / num_proc;
    int remain = N % num_proc;
    int start_row = rank * block;
    int end_row = (rank == num_proc - 1) ? start_row + block + remain : start_row + block;
    
    // 确保内存对齐
    float* aligned_A = (float*)_mm_malloc(N * N * sizeof(float), 16);
    for (int i = 0; i < N; i++) {
        memcpy(&aligned_A[i*N], A[i], N * sizeof(float));
    }

    for (int k = 0; k < N; k++) {
        // 处理主元行
        if (k >= start_row && k < end_row) {
            __m128 pivot = _mm_set1_ps(aligned_A[k*N + k]);
            
            // 向量化除法
            int j = k + 1;
            for (; j + 3 < N; j += 4) {
                __m128 row = _mm_load_ps(&aligned_A[k*N + j]);
                row = _mm_div_ps(row, pivot);
                _mm_store_ps(&aligned_A[k*N + j], row);
            }
            // 处理剩余元素
            for (; j < N; j++) {
                aligned_A[k*N + j] /= aligned_A[k*N + k];
            }
            aligned_A[k*N + k] = 1.0;
            
            // 广播更新后的行
            for (int p = 0; p < num_proc; p++) {
                if (p != rank) {
                    MPI_Send(&aligned_A[k*N], N, MPI_FLOAT, p, 2, MPI_COMM_WORLD);
                }
            }
        }
        else {
            MPI_Recv(&aligned_A[k*N], N, MPI_FLOAT, MPI_ANY_SOURCE, 2, 
                    MPI_COMM_WORLD, MPI_STATUS_IGNORE);
        }

        // 向量化更新其他行
        for (int i = max(start_row, k + 1); i < end_row; i++) {
            __m128 factor = _mm_set1_ps(aligned_A[i*N + k]);
            int j = k + 1;
            for (; j + 3 < N; j += 4) {
                __m128 a_ij = _mm_load_ps(&aligned_A[i*N + j]);
                __m128 a_kj = _mm_load_ps(&aligned_A[k*N + j]);
                __m128 res = _mm_sub_ps(a_ij, _mm_mul_ps(factor, a_kj));
                _mm_store_ps(&aligned_A[i*N + j], res);
            }
            // 处理剩余元素
            for (; j < N; j++) {
                aligned_A[i*N + j] -= aligned_A[i*N + k] * aligned_A[k*N + j];
            }
            aligned_A[i*N + k] = 0.0;
        }
    }

    // 将结果拷贝回原数组
    for (int i = 0; i < N; i++) {
        memcpy(A[i], &aligned_A[i*N], N * sizeof(float));
    }
    _mm_free(aligned_A);
}

void f_mpi_openmp()
{
    timeval t_start, t_end;
    int num_proc, rank;

    MPI_Comm_size(MPI_COMM_WORLD, &num_proc);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);

    int block = N / num_proc;
    int remain = N % num_proc;

    if (rank == 0) {
        reset_A(A, arr);
        gettimeofday(&t_start, NULL);
        
        // 任务划分 - 使用批量发送提高效率
        for (int i = 1; i < num_proc; i++) {
            int start = i * block;
            int count = (i == num_proc - 1) ? block + remain : block;
            MPI_Send(&A[start], count * N, MPI_FLOAT, i, 0, MPI_COMM_WORLD);
        }

        // OpenMP优化的LU分解
        LU_openmp(A, rank, num_proc);

        // 收集结果
        for (int i = 1; i < num_proc; i++) {
            int start = i * block;
            int count = (i == num_proc - 1) ? block + remain : block;
            MPI_Recv(&A[start], count * N, MPI_FLOAT, i, 1, 
                    MPI_COMM_WORLD, MPI_STATUS_IGNORE);
        }

        gettimeofday(&t_end, NULL);
        cout << "Block MPI LU with OpenMP time cost: "
             << 1000 * (t_end.tv_sec - t_start.tv_sec) +
            0.001 * (t_end.tv_usec - t_start.tv_usec) << "ms" << endl;
    }
    else {
        // 接收分配到的数据块
        int start = rank * block;
        int count = (rank == num_proc - 1) ? block + remain : block;
        MPI_Recv(&A[start], count * N, MPI_FLOAT, 0, 0, 
                MPI_COMM_WORLD, MPI_STATUS_IGNORE);

        // OpenMP优化的LU分解
        LU_openmp(A, rank, num_proc);

        // 发送结果回主进程
        MPI_Send(&A[start], count * N, MPI_FLOAT, 0, 1, MPI_COMM_WORLD);
    }
}

// OpenMP优化的LU分解函数
void LU_openmp(float** A, int rank, int num_proc)
{
    int block = N / num_proc;
    int remain = N % num_proc;
    int start_row = rank * block;
    int end_row = (rank == num_proc - 1) ? start_row + block + remain : start_row + block;

    for (int k = 0; k < N; k++) {
        // 串行处理主元行
        if (k >= start_row && k < end_row) {
            float pivot = A[k][k];
            #pragma omp parallel for
            for (int j = k + 1; j < N; j++) {
                A[k][j] /= pivot;
            }
            A[k][k] = 1.0;
            
            // 广播更新后的行给所有进程
            for (int p = 0; p < num_proc; p++) {
                if (p != rank) {
                    MPI_Send(&A[k], N, MPI_FLOAT, p, 2, MPI_COMM_WORLD);
                }
            }
        }
        else {
            MPI_Recv(&A[k], N, MPI_FLOAT, MPI_ANY_SOURCE, 2, 
                    MPI_COMM_WORLD, MPI_STATUS_IGNORE);
        }

        // 并行更新其他行
        #pragma omp parallel for
        for (int i = max(start_row, k + 1); i < end_row; i++) {
            float factor = A[i][k];
            for (int j = k + 1; j < N; j++) {
                A[i][j] -= factor * A[k][j];
            }
            A[i][k] = 0.0;
        }
    }
}

void LU_opt(float A[][N], int rank, int num_proc)
{
    __m128 t1, t2, t3;
    int block = N / num_proc;
    int remain = N % num_proc;
    int begin = rank * block;
    int end = rank != num_proc - 1 ? begin + block : begin + block + remain;
#pragma omp parallel num_threads(thread_count),private(t1, t2, t3)
    for (int k = 0; k < N; k++)
    {
        if (k >= begin && k < end)
        {
            float temp1[4] = { A[k][k], A[k][k], A[k][k], A[k][k] };
            t1 = _mm_loadu_ps(temp1);
#pragma omp for schedule(static)
            for (int j = k + 1; j < N - 3; j += 4)
            {
                t2 = _mm_loadu_ps(A[k] + j);
                t3 = _mm_div_ps(t2, t1);
                _mm_storeu_ps(A[k] + j, t3);
            }
            for (int j = N - N % 4; j < N; j++)
            {
                A[k][j] = A[k][j] / A[k][k];
            }
            A[k][k] = 1.0;
            for (int p = rank + 1; p < num_proc; p++)
                MPI_Send(&A[k], N, MPI_FLOAT, p, 2, MPI_COMM_WORLD);
        }
        else
        {
            int cur_p = k / block;
            if (cur_p < rank)
                MPI_Recv(&A[k], N, MPI_FLOAT, cur_p, 2, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
        }
        for (int i = begin; i < end && i < N; i++)
        {
            if (i >= k + 1)
            {
                float temp2[4] = { A[i][k], A[i][k], A[i][k], A[i][k] };
                t1 = _mm_loadu_ps(temp2);
#pragma omp for schedule(static)
                for (int j = k + 1; j <= N - 3; j += 4)
                {
                    t2 = _mm_loadu_ps(A[i] + j);
                    t3 = _mm_loadu_ps(A[k] + j);
                    t3 = _mm_mul_ps(t1, t3);
                    t2 = _mm_sub_ps(t2, t3);
                    _mm_storeu_ps(A[i] + j, t2);
                }
                for (int j = N - N % 4; j < N; j++)
                    A[i][j] = A[i][j] - A[i][k] * A[k][j];
                A[i][k] = 0;
            }
        }
    }
}


void f_mpi_opt()
{

    timeval t_start;
    timeval t_end;

    int num_proc;//进程数
    int rank;//识别调用进程的rank，值从0~size-1

    MPI_Comm_size(MPI_COMM_WORLD, &num_proc);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);

    int block = N / num_proc;
    int remain = N % num_proc;

    //0号进程——任务划分
    if (rank == 0)
    {
        reset_A(A, arr);
        gettimeofday(&t_start, NULL);
        //任务划分
        for (int i = 1; i < num_proc; i++)
        {
            if (i != num_proc - 1)
            {
                for (int j = 0; j < block; j++)
                    MPI_Send(&A[i * block + j], N, MPI_FLOAT, i, 0, MPI_COMM_WORLD);
            }
            else
            {
                for (int j = 0; j < block + remain; j++)
                    MPI_Send(&A[i * block + j], N, MPI_FLOAT, i, 0, MPI_COMM_WORLD);
            }
        }
        LU_opt(A, rank, num_proc);
        //处理完0号进程自己的任务后需接收其他进程处理之后的结果
        for (int i = 1; i < num_proc; i++)
        {
            if (i != num_proc - 1)
            {
                for (int j = 0; j < block; j++)
                    MPI_Recv(&A[i * block + j], N, MPI_FLOAT, i, 1,
                        MPI_COMM_WORLD, MPI_STATUS_IGNORE);
            }
            else
            {
                for (int j = 0; j < block + remain; j++)
                    MPI_Recv(&A[i * block + j], N, MPI_FLOAT, i, 1,
                        MPI_COMM_WORLD, MPI_STATUS_IGNORE);
            }
        }
        gettimeofday(&t_end, NULL);
        cout << "Block MPI LU with SSE and OpenMP time cost: "
            << 1000 * (t_end.tv_sec - t_start.tv_sec) +
            0.001 * (t_end.tv_usec - t_start.tv_usec) << "ms" << endl;
        //print_A(A);
    }

    //其他进程
    else
    {
        //非0号进程先接收任务
        if (rank != num_proc - 1)
        {
            for (int j = 0; j < block; j++)
                MPI_Recv(&A[rank * block + j], N, MPI_FLOAT, 0, 0,
                    MPI_COMM_WORLD, MPI_STATUS_IGNORE);
        }
        else
        {
            for (int j = 0; j < block + remain; j++)
                MPI_Recv(&A[rank * block + j], N, MPI_FLOAT, 0, 0,
                    MPI_COMM_WORLD, MPI_STATUS_IGNORE);
        }
        LU_opt(A, rank, num_proc);
        //处理完后向零号进程返回结果
        if (rank != num_proc - 1)
        {
            for (int j = 0; j < block; j++)
                MPI_Send(&A[rank * block + j], N, MPI_FLOAT, 0, 1, MPI_COMM_WORLD);
        }
        else
        {
            for (int j = 0; j < block + remain; j++)
                MPI_Send(&A[rank * block + j], N, MPI_FLOAT, 0, 1, MPI_COMM_WORLD);
        }
    }
}

int main()
{
    init_A(arr);

    //f_ordinary();

    MPI_Init(NULL, NULL);

    f_mpi();
    f_mpi_sse();
    f_mpi_openmp();
    f_mpi_opt();
    MPI_Finalize();
    
    return 0;
}
